<!doctype html>
<html lang="en">

<head>
<meta charset="utf-8" />
<meta http-equiv="Content-type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="description" content="Musings on the recent rash of attempts to unseat the Adam optimizer from being the workhorse of training transformer neural networks.">


<link rel="apple-touch-icon" sizes="180x180" href="https://lukemerrick.com/static//apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lukemerrick.com/static//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://lukemerrick.com/static//favicon-16x16.png">
<link rel="manifest" href="https://lukemerrick.com/static//site.webmanifest">
<title>First-Order Optimizer Fever</title>

<!-- Milligram for Styling, plus some custom rules and CodeHilite -->
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300italic,400,400italic,700,700italic">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.css" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/milligram/1.4.0/milligram.css" type="text/css">
<link rel="stylesheet" href="https://lukemerrick.com/static/main.css" type="text/css">
<link rel="stylesheet" href="https://lukemerrick.com/static/codehilite.css" type="text/css">

<!-- Beam Analytics. -->
<script src="https://beamanalytics.b-cdn.net/beam.min.js" data-token="fa8a85a8-15e1-443a-96de-cb77ec3d19cb" async></script>
</head>

<body>
<header class="header">
<nav>
<!-- <a href="https://lukemerrick.com/">Home</a> / -->
<a href="https://lukemerrick.com/">About Me</a> /
<a href="https://lukemerrick.com/posts">Posts</a>
<!-- <a href="https://lukemerrick.com/about.html">About Me</a> -->
</nav>
</header>
<article>
<p>2025-07-12</p>
<!-- BEGIN GENERATED FROM MARKDOWN -->
<h1>First-Order Optimizer Fever</h1>
<p><img alt="distracted boyfriend meme about different optimizers beyond AdamW" src="static/first_order_optimization/optimizer_meme.jpg"></p>
<p><strong>Welcome to the long-form blog-post version of this meme.</strong></p>
<p>Ever since Kingma and Ba dropped their evergreen banger <a href="https://arxiv.org/abs/1412.6980"><em>Adam: A Method for Stochastic Optimization</em> in December 2014</a>, AI researchers have been struggling hard to invent new and better algorithms for training neural networks. With the notable exception of a tweak to Adam's weight decay rule introduced in 2017 as <a href="https://arxiv.org/abs/1711.05101">AdamW</a>, a sea of new algorithms have come and gone without unseating Adam from its throne (despite the numerous lofty claims made by the corresponding publications).</p>
<p>Ever hopeful, however, AI practitioners have been continually eyeing and trying the latest practical optimizer research, and it seems that the past year or so has represented a real change in the state of the art of training big neural networks.</p>
<h2>Pragmatic Breakthroughs</h2>
<p><img alt="screenshot of the Kimi2 release blogpost highlighting their use of Muon" src="static/first_order_optimization/kimi_k2_screenshot.png"></p>
<p>With Moonshot bragging in the <a href="https://moonshotai.github.io/Kimi-K2/">release blog post for their Kimi K2 model</a> about the stability benefits of using a non-AdamW optimizer on a trillion-parameter model trained on trillions of tokens of data, it has suddenly become pretty difficult to argue that no real progress is happening in optimization of large transformer networks beyond Adam tweaks.</p>
<p>This development raises an important question: Why is today's progress more real than the not-so-real progress claimed in the sea of papers published in late 2010s and early 2020s? I believe the answer to this question lies in a shift of focus for the AI research community as a whole. As recent years have witnessed a shift towards scaling up the training of very large transformer models, the goalposts for optimization researchers have subtly but importantly shifted from the general objective of "making a better version of AdamW" toward the specific objective of "making something that works better than AdamW for training this giant transformer neural network on next-token-prediction." This is an important shift from the theoretical to the practical, and researchers are now taking advantage of a new route to impactful progress by leveraging empiricism to craft optimizers that directly tackle the "train a large transformer" problem.</p>
<h3>Muon, Muon, Muon</h3>
<p><img alt="so hot right now meme about Muon" src="static/first_order_optimization/muon_so_hot_meme.png"></p>
<p>Muon is perhaps the leading example of how empirically-motivated breakthroughs targeted at improving specific training workloads are actually starting to make real headway against the formidable AdamW baseline. Muon made a splash by powering impressive gains on <a href="https://github.com/KellerJordan/modded-nanogpt">NanoGPT speed runs</a> and is having a big moment this week as the Kimi K2 model promotes Moonshot's research <a href="https://arxiv.org/abs/2502.16982v1"><em>Muon is Scalable for LLM Training</em></a>. However, other researchers are also joining the party. Numerous groups are either building on top of Muon's neural-network-aware approach (e.g. <a href="https://arxiv.org/abs/2505.13416"><em>Gluon: Making Muon &amp; Scion Great Again!</em></a>) or taking an empiricist approach of their own by adjusting their algorithms based on transformer-training benchmarks rather than theoretical clarity alone (e.g. <a href="https://arxiv.org/abs/2506.07254"><em>A Stable Whitening Optimizer for Efficient Neural Network Training</em></a> and <a href="https://arxiv.org/abs/2505.21829"><em>In Search of Adam's Secret Sauce</em></a>). If you are an optimization researcher and you have not tried an empiricists approach focused on training transformers, now might be a good time to try it out -- it seems quite effective!</p>
<h2>Breakthroughs In Clarity</h2>
<p>I hope by now that I have convinced you recent progress in practical neural network optimization appears quite real and valuable. I'm not done, though -- I also want to convince you that we're in for a progress on the theory front as well.</p>
<p>There is a somewhat famous story of how photography professor Jerry Uelsmann found that grading students on the quantity of photos produced actually ended up causing them to produce the highest quality photos of anyone in the class (in other words, being strongly incentivized to embrace a process of trial and error helped accelerate their development as artists). I believe something similar is happening in optimization research right now, with empirically-motivated research leading to greater theoretical clarity.</p>
<p>For example, I claim that Muon kingpin Jeremy Bernstein is not operating at a crappier level of theoretical depth because of his focus on GPT speed runs. Rather, I believe he's actually actually deepening his focus onto the parts of the theory that actually matter as highlighted by experimental feedback, allowing him to make the theoretical contributions that actually matter to the field. You can see this phenomenon in a <a href="https://youtu.be/4OAiakkmKQs">recent talk he gave on first-order optimization</a>: The clarity and generality of the material far surpasses anything I had in my studies as a university student.</p>
<p><img alt="a screen capture from a video lecture given by Jeremy Bernstein on first order optimization" src="static/first_order_optimization/jeremy_bernstein_lecture.png">
<em>I would have paid good money for a slide like this to show up in my optimization class in college.</em></p>
<p>It's not just the Muon hype train that's benefitting from this approach, other groups are catching on. The SPlus paper [8] is full of clear explanations of theoretically deep material while also openly admitting that the core improvements of their new algorithm were largely derived from an experimental rather than analytical approach. </p>
<p><img alt="excerpt from the SPlus optimizer paper" src="static/first_order_optimization/splus_excerpt.png"></p>
<h2>Conclusion</h2>
<p>The era of AdamW as the dominant paradigm for optimizing training giant transformer neural networks over giant datasets may be coming to a close. Though extensive empirical experimentation is a key ingredient in several of the most exciting challenger algorithms, the experimentally-informed papers coming out in recent months are some of the most theoretically clear and thoughtful that I have seen in years. Now is a wonderful time to turn your head back and check out some of these exciting new developments, and unlike the meme, your AdamW optimizer will not actually be jealous of the lost attention.</p>
<!-- EMD GENERATED FROM MARKDOWN -->
</article>

<!-- MathJax for Math Typesetting -->
<script src="https://lukemerrick.com/static/load-mathjax.js" async></script>
</body>

</html>