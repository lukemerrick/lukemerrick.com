<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <meta name="description" content="Exploration of least squares linear regression modeling in an online setting.">
    
    
    <link rel="shortcut icon" type="image/png" href="https://lukemerrick.com/static/favicon.png" />
    <title>Recursive Least Squares Derivation</title>

    <!-- Milligram for Styling -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300italic,400,400italic,700,700italic">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.css" type="text/css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/milligram/1.4.0/milligram.css" type="text/css">
    <link rel="stylesheet" href="https://lukemerrick.com/static/main.css" type="text/css">

    <!-- Plausible.io, a simple and privacy-friendly alternative to Google Analytics. Self-hosted. -->
    <script async defer data-domain="lukemerrick.com" src="https://tinybox.lukemerrick.com/js/plausible.js"></script>

</head>

<body>
    <header class="header">
<nav>
    <!-- <a href="https://lukemerrick.com/">Home</a> / -->
    <a href="https://lukemerrick.com/">About Me</a> /
    <a href="https://lukemerrick.com/posts">Posts</a>
    <!-- <a href="https://lukemerrick.com/about.html">About Me</a> -->
</nav>
</header>
    <article>
        <p>2020-07-09</p>
        <!-- BEGIN GENERATED FROM MARKDOWN -->
        <h1>Recursive Least Squares for ML</h1>
        <h2>Updates</h2>
        <ul>
        <li>2020.10.15 -- I rewrote a lot of this post. The motivation is generic and expanded, and the math has been massaged a bit. This makes <span class="arithmatex">\(\ell_2\)</span> regularization now included in both sets of recursive equations, as well as giving expressions that are far less numerically unstable.</li>
        </ul>
        <h2>Why I'm writing this</h2>
        <p>Recently I've been working with the recursive formulation of least-squares regression. At the time of writing, there are a lot of solid resources for the signal processing view of Recursive Least Squares (like <a href="https://en.wikipedia.org/wiki/Recursive_least_squares_filter">Wikipedia's RLS article</a>), but I was unable to find a good write-up from the machine learning view (e.g. as a <a href="https://arxiv.org/abs/1805.08136">differentiable closed-form solver</a>). This is my attempt to frame things in concepts and language friendlier to machine-learning-minded people like myself.</p>
        <h2>Pre-requisite knowledge</h2>
        <p>To follow this post, you probably will want to be somewhat familiar with the following topics:</p>
        <ul>
        <li>Supervised machine learning</li>
        <li>Basic linear algebra (working with matrices and vectors)</li>
        <li>Linear regression models</li>
        </ul>
        <p>Additionally, familiarity with the following will probably enhance your understanding:</p>
        <ul>
        <li>Computational complexity (big-<span class="arithmatex">\(O\)</span> notation)</li>
        <li>Convex optimization (or at least the multivariable/matrix calculus that goes into it)</li>
        </ul>
        <h2>The ML motivation and context for RLS</h2>
        <p><em>Skip this (long) section if you are more interested in the RLS algorithm than its application in ML (you can always come back!).</em></p>
        <p>In the simplest settings of supervised learning, we usually think of a "machine learning problem" as finding a good way to approximate the mapping between input and output for a fixed dataset of observed input-output pairs. Most supervised learning algorithms boil down to coming up with some structure for the input-to-output mapping function with adjustable parameters (e.g. linear models, tree-based models, neural nets, etc.), and then efficiently finding a specific set of parameters which give rise to low average error between our function's output and the true outputs in the observed dataset. Somewhat confusingly, we use the word <em>model</em> to refer to the combination of a functional form and a specific set of parameters, not just the functional form (though in statistical learning theory, we call the functional form by itself the <em>hypothesis class</em> and the selected model a <em>hypothesis</em>, which I find much clearer). In more theoretical terms, this is the approach of directly applying the <a href="https://en.wikipedia.org/wiki/Empirical_risk_minimization">Empirical Risk Minimization (ERM) principle</a> to a fixed dataset to find a good approximation of the input-to-output function. </p>
        <p>In many real-world settings, however, our dataset is far from a static collection of observations. Data is usually generated by an ongoing process, and that process can change. There are a lot of terms for this change -- dataset shift, covariate shift, concept drift, and more. In a nutshell, though, we end up in a situation in which it's best to update our approximation of the input-output function as more data becomes available.</p>
        <p>There are a number of ML subfields that provide useful formulations of problems like this. Online Learning and Reinforcement Learning are possibly the most popular and well-established of these. Online Learning re-formulates the problem at hand as finding a function which minimizes the total error when faced with a sequence of many (potentially adversarial) error responses given in response to the current model's prediction. Often the goal is to find a sequence of model functions at each time point whose errors converge to being no worse than the errors that the best possible single model would give rise to. Reinforcement Learning re-formulates the problem as back-and-forth interaction between an <em>agent</em> and an <em>environment</em>.  The task of learning an input-to-output model turns into the task of learning a <em>policy</em> which maps <em>state</em> to <em>action</em>, while the <em>environment</em> maps that action to a <em>reward</em> and a new <em>state</em>. A good <em>policy</em> is one which achieves a high cumulative reward. </p>
        <p>Unfortunately, these subfields' approaches are often overkill for what we see in the real world -- a slight overstepping of the traditional bounds of supervised learning due to an ever-growing dataset and a dash of dataset shift. Why are OL and RL overkill? Online Learning is concerned with a possibly adversarial response and focuses on minimizing regret compared to a single good model used for all time. This makes it an awkward match for many not-so-adversarial data-generating processes, where it can be better to worry about low absolute error than low regret. Reinforcement Learning brings to bear even more mathematical machinery to deal with the way the world reacts to the outputs of our model. When we know the world doesn't particularly care about our predictions, this extra machinery becomes cumbersome.</p>
        <p>Faced with the reality of ML problems that slip between the cracks of sub-disciplines, practitioners often turn to some kind of <em>batch learning</em> approach in which they periodically gather a fresh dataset that includes recently-generated data and then learn a new model from scratch using this new dataset. In other cases, especially "big data" applications like ad-tech, practitioners will instead turn to computationally efficient streaming algorithms (which are sometimes called "online" algorithms just to confuse us!). These algorithms can make small updates to the parameters of the input-output approximation function every time a new data point is collected -- all without referring back to historical data. These streaming algorithms often guarantee that the model at any time will be similar to the model one would get by training a batch version of the same algorithm on a large dataset containing all data since the beginning of the streaming. RLS can be seen as one such streaming algorithm. It's certainly not the only one, nor the most popular one, but it does offer a number of compelling properties.</p>
        <h3>Why RLS?</h3>
        <p>RLS let's us produce the same sequence of linear models we would get if we batch-learned a new model on our entire cumulative dataset every time a new data point is added, but using about the same amount of computation as fitting just the final model on all the data. Whenever a new data point is acquired, the model for <span class="arithmatex">\(i+1\)</span> points can be computed quickly and cheaply using the <span class="arithmatex">\(i\)</span>th model, without looking at any data besides the new point. </p>
        <p>Compared to online gradient descent, RLS is more computationally costly and is only defined for squared loss (so no generalized linear models like logistic regression). On the plus side, though, RLS is truly a closed-form solver. It doesn't require tuning a learning rate, and it is guaranteed to output the exact model parameters which would be given by fitting linear regression on all the data in one go.</p>
        <p>By way of the signal-processing literature, RLS also comes with a <em>forgetting factor</em>, which we can use to exponentially down-weight older examples (effectively to zero, far enough back in history). This lets RLS be somewhat analogous to a batch learning approach which drops old "stale" data points.</p>
        <h2>The ML setting for RLS</h2>
        <h3>The data</h3>
        <p>We consider a process that over time generates a sequence of vectors containing predictive information, as well as corresponding scalar measurements describing outcomes we'd like to predict using the information in the vectors. We'll notate these sequences as follows.</p>
        <p><span class="arithmatex">\(\pmb{x}_1, \pmb{x}_2, \pmb{x}_3, \ldots\)</span>  and <span class="arithmatex">\(y_1, y_2, y_3, \ldots\)</span></p>
        <p>For example, perhaps <span class="arithmatex">\(\pmb{x}_i\)</span> summarizes pertinent sports statistics for a team just before its <span class="arithmatex">\(i\)</span>th game, while <span class="arithmatex">\(y_i\)</span> is that team's score in the <span class="arithmatex">\(i\)</span>th game. We'd like to predict that team's score before they play, using the statistics available before the game.</p>
        <h3>The ML objective</h3>
        <p>We seek a system that can predict outcome measurements using information vectors, <em>with minimal error.</em> While ultimately we care about the <span class="arithmatex">\(i\)</span>th model's prediction error on the <span class="arithmatex">\(i\)</span>th outcome, when it comes to training we have to settle for using relevant historical examples to quantify the error we want to minimize.</p>
        <h2>Design choices for RLS</h2>
        <p>In this setting, there are many model structures we could use, many error metrics we could use, and many ways we could decide on relevant historical examples. RLS arises as the solution in this setting when we make a certain set of "design choices."</p>
        <ul>
        <li>First, we decide to use <span class="arithmatex">\(\ell_2\)</span> (euclidean) distance between a prediction and a true outcome as the measurement of error.</li>
        <li>Second, when computing average error, we weight our average using weights which decay exponentially going back in time. This is our way of defining the relevance in historical examples, and it uses recency as a proxy for relevance. </li>
        <li>Last, we will use a linear model structure, assuming <span class="arithmatex">\(y_i = \pmb{\theta}_i ^\intercal \pmb{x}_i + \epsilon\)</span> (where <span class="arithmatex">\(\pmb{\theta}_i\)</span> is a vector of model parameters and <span class="arithmatex">\(\epsilon\)</span> is unpredictable noise).</li>
        </ul>
        <p>Putting this all together, the <span class="arithmatex">\(i\)</span>th model is defined by a parameter vector <span class="arithmatex">\(\pmb{\theta}_i\)</span> which minimizes exponentially-weighted average error, which we'll call <span class="arithmatex">\(L_i(\pmb{\theta})\)</span>. To make the notation simpler, we'll use the total error rather than the average, but the difference is only a constant that doesn't affect the minimization.</p>
        <p><span class="arithmatex">\(\pmb{\theta}_i = \underset{\pmb{\theta}}{\operatorname{argmin}}~L_i(\pmb{\theta}) = \underset{\pmb{\theta}}{\operatorname{argmin}}~\sum_{t=1}^i \lambda^{i - t} (y_t - \pmb{\theta}^\intercal\pmb{x}_t)^2\)</span></p>
        <h3>Generalizing to <span class="arithmatex">\(\ell_2\)</span> regularization</h3>
        <p>One of two generalizations I add in this derivation that I didn't find in RLS derivations on the internet is <span class="arithmatex">\(\ell_2\)</span> regularization. In many cases, ML folks have found that constraining the <span class="arithmatex">\(\ell_2\)</span> magnitude of a linear model's parameters can improve overfitting (and also some of the matrix inversions used to compute the parameters). Using a Lagrange multiplier <span class="arithmatex">\(\alpha\)</span> to incorporate the typical <span class="arithmatex">\(\ell_2\)</span> norm constraint into the loss function, we get an expanded definition of <span class="arithmatex">\(L_i(\pmb{\theta})\)</span>.</p>
        <p><span class="arithmatex">\(L_i(\pmb{\theta}) = \sum_{t=1}^i \lambda^{i - t} (y_t - \pmb{\theta}^\intercal\pmb{x}_t)^2 + \alpha ||\pmb{\theta}||_2^2\)</span></p>
        <p>Since we use total error rather than average error in a case in which the dataset grows, we need to break up the regularization term into something added to each term in the sum. To do this, we'll use a new <span class="arithmatex">\(\ell_2\)</span> penalty term <span class="arithmatex">\(\beta\)</span>.</p>
        <p><span class="arithmatex">\(L_i(\pmb{\theta}) = \sum_{t=1}^i \lambda^{i - t} \left( (y_t - \pmb{\theta}^\intercal\pmb{x}_t)^2 + \beta ||\pmb{\theta}||_2^2\right)\)</span>, where <span class="arithmatex">\(\alpha = \beta \sum_{t=1}^i \lambda^{i - t}\)</span></p>
        <p>By defining <span class="arithmatex">\(\beta\)</span> in this way, we see that <span class="arithmatex">\(\alpha\)</span> does not stay fixed as <span class="arithmatex">\(i\)</span> increases, which seems odd. However, this actually accounts for the fact that we're using the weighted sum of error rather the weighted average.</p>
        <h2>Deriving non-recursive least squares</h2>
        <p>Let's start by deriving the <span class="arithmatex">\(\pmb{\theta} = (X^\intercal W  X)^{-1}X^\intercal W Y\)</span> weighted linear regression closed-form solution, but with a number of non-standard choices in expressions to account for <span class="arithmatex">\(\ell_2\)</span> regularization.</p>
        <p>First, some definitions.</p>
        <p><span class="arithmatex">\(\pmb{x}_i \in \mathbb{R}^d, \pmb{\theta}_i \in \mathbb{R}^d\)</span></p>
        <p><span class="arithmatex">\(\pmb{w}'_i = \begin{bmatrix}
        \lambda^{i-1} &amp; \lambda^{i-2} &amp; \cdots &amp; \lambda &amp; 1
        \end{bmatrix}^\intercal, W'_i = \operatorname{diag}(\pmb{w}'_i)\)</span></p>
        <p><span class="arithmatex">\(X'_i = \begin{bmatrix}
        \pmb{x}_1 &amp; \pmb{x}_2 &amp; \cdots &amp; \pmb{x}_i
        \end{bmatrix}^\intercal,
        Y'_i = \begin{bmatrix}
        y_1 &amp; y_2 &amp; \cdots &amp; y_i
        \end{bmatrix}^\intercal\)</span></p>
        <p>Note the primes in the above names. We're actually going to put our <span class="arithmatex">\(\ell_2\)</span> regularization right into the <span class="arithmatex">\(X\)</span>, <span class="arithmatex">\(Y\)</span>, and <span class="arithmatex">\(W\)</span> matrices (see this <a href="https://stats.stackexchange.com/a/164546">CrossValidated post</a> for inspiration). Not only does this new notation keep the equations short, it also makes life a lot easier when we derive the recursive formulas.</p>
        <p>We note that <span class="arithmatex">\(\beta \operatorname{Tr}(W_i')\)</span> is our <span class="arithmatex">\(\ell_2\)</span> regularization Lagrange multiplier at time <span class="arithmatex">\(i\)</span>. We'll append <span class="arithmatex">\(d\)</span> of these values on the end of our weight vector. We'll also pad <span class="arithmatex">\(Y'\)</span> with <span class="arithmatex">\(d\)</span> zeros, and <span class="arithmatex">\(X'\)</span> with the <span class="arithmatex">\(d \times d\)</span> identity matrix. When <span class="arithmatex">\(\beta = 0\)</span>, all of these additions are zero and have no effect on the average error or its gradient.</p>
        <p><span class="arithmatex">\(v_i = \beta \operatorname{Tr}(W'_i)\)</span></p>
        <p><span class="arithmatex">\(\pmb{v}_i = \begin{bmatrix}
        v_i &amp; v_i &amp; \cdots &amp; v_i
        \end{bmatrix}^\intercal\)</span>, <span class="arithmatex">\(\pmb{v}_i \in \mathbb{R}^d\)</span></p>
        <p><span class="arithmatex">\(\pmb{0} = \begin{bmatrix} 0 &amp; 0 &amp; \cdots &amp; 0 \end{bmatrix}^\intercal\)</span>, <span class="arithmatex">\(\pmb{0} \in \mathbb{R}^d\)</span></p>
        <p><span class="arithmatex">\(\pmb{w}_i = \begin{bmatrix} \pmb{w}'_i \\ \pmb{v}_i \end{bmatrix},
        W_i = \operatorname{diag}(\pmb{w}_i)\)</span></p>
        <p><span class="arithmatex">\(X_i = \begin{bmatrix} X'_i \\ I \end{bmatrix}, Y_i = \begin{bmatrix} Y'_i \\ \pmb{0} \end{bmatrix}\)</span></p>
        <p>Now we can define our loss in matrix notation, and see that the <span class="arithmatex">\(\ell_2\)</span> regularization term has been pushed inside the new matrices.</p>
        <p><span class="arithmatex">\(L_i(\pmb{\theta}) = (Y'_i - X'_i \pmb{\theta})^\intercal W'_i (Y'_i - X'_i \pmb{\theta}) + \beta \operatorname{Tr}(W'_i) ||\pmb{\theta}||_2^2\)</span></p>
        <p><span class="arithmatex">\(= (Y'_i - X'_i \pmb{\theta})^\intercal W'_i (Y'_i - X'_i \pmb{\theta}) + \beta \operatorname{Tr}(W'_i) \pmb{\theta}^\intercal\pmb{\theta}\)</span></p>
        <p><span class="arithmatex">\(= (Y_i - X_i \pmb{\theta})^\intercal W_i (Y_i - X_i \pmb{\theta})\)</span></p>
        <h3>Closed-form least-squares solution</h3>
        <p>By first order optimality conditions, <span class="arithmatex">\(\nabla L_i(\pmb{\theta}_i) = 0\)</span>. We can use this fact to derive a closed-form solution for <span class="arithmatex">\(\pmb{\theta}_i\)</span>.</p>
        <p><span class="arithmatex">\(\nabla L_i(\pmb{\theta}_i) = -2 X_i ^\intercal W_i Y_i + 2 X_i ^\intercal W_i X_i \pmb{\theta}_i = 0\)</span></p>
        <p><span class="arithmatex">\(X_i ^\intercal W_i X_i \pmb{\theta}_i = X_i ^\intercal W_i Y_i\)</span></p>
        <p><span class="arithmatex">\(\pmb{\theta}_i = (X_i ^\intercal W_i X_i )^{-1} X_i ^\intercal W_i Y_i\)</span></p>
        <p>We can also see what this looks like in the conventional definition of <span class="arithmatex">\(X, Y, W\)</span>.</p>
        <p><span class="arithmatex">\(\pmb{\theta}_i = 
        \left(
        \begin{bmatrix} X'_i~^\intercal &amp; I \end{bmatrix}
        \operatorname{diag}\left(\begin{bmatrix} \pmb{w}'_i \\ \pmb{v}_i \end{bmatrix}\right)
        \begin{bmatrix} X'_i \\ I \end{bmatrix}
        \right)^{-1}
        \begin{bmatrix} X'_i~^\intercal &amp; I \end{bmatrix} \operatorname{diag} \left(\begin{bmatrix} \pmb{w}'_i \\ \pmb{v}_i \end{bmatrix}\right)
        \begin{bmatrix} Y'_i &amp; \pmb{0} \end{bmatrix}\)</span></p>
        <p><span class="arithmatex">\(= (X'_i ~^\intercal W'_i X'_i + \beta \operatorname{Tr}(W'_i) I)^{-1} X'_i ~^\intercal W'_i Y'_i\)</span></p>
        <p>Lastly, we'll simply the expression in terms of <span class="arithmatex">\(R_i = X_i ^\intercal W_i X_i\)</span> and <span class="arithmatex">\(P_i = R_i^{-1}\)</span>. We shall refer to <span class="arithmatex">\(R_i\)</span> as the covariance matrix, since when <span class="arithmatex">\(\beta =0\)</span> and <span class="arithmatex">\(X_i\)</span> has been shifted to have zero mean in all dimensions, it represents the un-normalized time-weighted sample covariance matrix. <span class="arithmatex">\(P_i\)</span> is called the precision matrix, as it is the inverse of the covariance matrix.</p>
        <p><span class="arithmatex">\(\pmb{\theta}_i = P_i X_i ^\intercal W_i Y_i\)</span></p>
        <h2>Deriving a Recursive Solution</h2>
        <p>As <span class="arithmatex">\(i\)</span> increases, so does the size of <span class="arithmatex">\(X_i\)</span>, <span class="arithmatex">\(Y_i\)</span>, as well as the cost of computing <span class="arithmatex">\(\pmb{\theta}_i\)</span> from them. This is not desirable!  The computational complexity is something like <span class="arithmatex">\(\mathcal{O}(d^2n + d^3)\)</span> due to the ever-growing matrix multiplication to compute <span class="arithmatex">\(R_i\)</span> and the matrix-inverse to invert it. We'd like to slice that down to something much lighter so we can run it frequently as new data becomes available. What we'd like is a recursive expression defining <span class="arithmatex">\(\pmb{\theta}_{i+1}\)</span> in terms of <span class="arithmatex">\(\pmb{x}_{i+k}\)</span>, <span class="arithmatex">\(y_{i+k}\)</span>, and <span class="arithmatex">\(\pmb{\theta}_i\)</span>, so that the computation doesn't grow with <span class="arithmatex">\(n\)</span>. Luckily, it turns out we can get just that if we keep track of the covariance or precision matrix as well as the parameter vector.</p>
        <h3>Generalizing to batch updates</h3>
        <p>The second of two generalizations I add in this derivation that I didn't find in RLS derivations on the internet is batch updating. In many settings, it may actually be desirable update the parameter vector for more than one new point at once. For example, if database updates are done in batches to optimize performance and many new points become available simultaneously, or if we don't expect the parameter vector to change much point-to-point and we can run a batch update in a single call to an optimized linear algebra library much faster than we could run iterative updates in a loop.</p>
        <p>The generalization also happens to not add too much complexity. We just express <span class="arithmatex">\(\pmb{\theta}_{i+k}\)</span> in terms of <span class="arithmatex">\(\pmb{\theta}_i\)</span> rather than <span class="arithmatex">\(\pmb{\theta}_{i+1}\)</span> in terms of <span class="arithmatex">\(\pmb{\theta}_i\)</span>. To keep the notation clean in batch form, we'll define an implicitly-indexed notation for matrices describing <span class="arithmatex">\(k\)</span> recent examples.</p>
        <p><span class="arithmatex">\(X' = \begin{bmatrix}
        \pmb{x}_{i + 1} &amp; \pmb{x}_{i+2} &amp; \cdots &amp; \pmb{x}_{i+k}
        \end{bmatrix}^\intercal,
        Y' = \begin{bmatrix}
        y_{i + 1} &amp; y_{i + 2} &amp; \cdots &amp; y_{i + k}
        \end{bmatrix}^\intercal,
        \pmb{w}' = \begin{bmatrix}
        \lambda^{k - 1} &amp; \lambda^{k - 2} &amp; \cdots &amp; 1
        \end{bmatrix}^\intercal,
        \pmb{v} = \pmb{v}_k\)</span></p>
        <p><span class="arithmatex">\(\pmb{w} = \begin{bmatrix} \pmb{w}' \\ \pmb{v} \end{bmatrix},
        W = \operatorname{diag}(\pmb{w})\)</span></p>
        <p><span class="arithmatex">\(X = \begin{bmatrix} X' \\ I \end{bmatrix}, Y = \begin{bmatrix} Y' \\ \pmb{0} \end{bmatrix}\)</span></p>
        <p><span class="arithmatex">\(R = X ^\intercal W X\)</span></p>
        <p>This lets us cleanly express <span class="arithmatex">\(X_{i+k}\)</span>, <span class="arithmatex">\(Y_{i+k}\)</span>, and <span class="arithmatex">\(W_{i+k}\)</span> recursively.</p>
        <p><span class="arithmatex">\(\pmb{v}_{i+k} = \lambda^k \pmb{v}_i + \pmb{v}\)</span></p>
        <p><span class="arithmatex">\(\pmb{w}_{i+k} = \begin{bmatrix} \lambda^k \pmb{w}'_{i} \\ \pmb{w'} \\ \pmb{v}_{i+k} \end{bmatrix},
        W_{i+k} = \operatorname{diag}(\pmb{w}_{i+k})\)</span></p>
        <p><span class="arithmatex">\(X_{i+k} = \begin{bmatrix} X'_i \\ X' \\ I \end{bmatrix}, Y_{i+k} = \begin{bmatrix} Y'_i \\  Y' \\ \pmb{0} \end{bmatrix}\)</span></p>
        <h3>The recursion relationship for <span class="arithmatex">\(\pmb{\theta}\)</span></h3>
        <p>First we show that <span class="arithmatex">\(R_{i+k} = \lambda^k R_i + R\)</span>.</p>
        <p><span class="arithmatex">\(R_{i + k} = X_{i+k} ^\intercal W_{i+k} X_{i+k}\)</span></p>
        <p><span class="arithmatex">\(=
        \begin{bmatrix} X'_{i}~^\intercal &amp; X'~^\intercal &amp; I \end{bmatrix}
        \operatorname{diag}\left(\begin{bmatrix} \lambda^k \pmb{w}'_{i} \\ \pmb{w'} \\ \pmb{v}_{i+k} \end{bmatrix}\right)
        \begin{bmatrix} X'_i \\ X' \\ I \end{bmatrix}\)</span></p>
        <p><span class="arithmatex">\(= X'_i~^\intercal \operatorname{diag}(\lambda^k \pmb{w}'_{i}) X'_i + X'~^\intercal \operatorname{diag}(\pmb{w'})X' + I \operatorname{diag}(\lambda^k \pmb{v}_i + \pmb{v})I\)</span></p>
        <p><span class="arithmatex">\(= \lambda^k(X'_i~^\intercal \operatorname{diag}(\pmb{w}'_{i}) X'_i + I \operatorname{diag}(\pmb{v}_i)I) + (X'~^\intercal \operatorname{diag}(\pmb{w}')X' + I \operatorname{diag}(\pmb{v})I)\)</span></p>
        <p><span class="arithmatex">\(= \lambda^k X_i~^\intercal W_i X_i + X~^\intercal W X\)</span></p>
        <p><span class="arithmatex">\(=\lambda^k R_i + R\)</span></p>
        <p>Then we look at the equation for <span class="arithmatex">\(\pmb{\theta}_{i+k}\)</span>.</p>
        <p><span class="arithmatex">\(\pmb{\theta}_{i+k} = P_{i+k} X_{i+k} ^\intercal W_{i+k} Y_{i+k}\)</span></p>
        <p><span class="arithmatex">\(= P_{i+k} \begin{bmatrix} X'_{i}~^\intercal &amp; X'~^\intercal &amp; I \end{bmatrix}
        \operatorname{diag}\left(\begin{bmatrix} \lambda^k \pmb{w}'_{i} \\ \pmb{w'} \\ \pmb{v}_{i+k} \end{bmatrix}\right) \begin{bmatrix} Y'_i \\  Y' \\ \pmb{0} \end{bmatrix}\)</span></p>
        <p><span class="arithmatex">\(= P_{i+k} \begin{bmatrix} X'_{i}~^\intercal &amp; X'~^\intercal \end{bmatrix}
        \operatorname{diag}\left(\begin{bmatrix} \lambda^k \pmb{w}'_{i} \\ \pmb{w'} \end{bmatrix}\right) \begin{bmatrix} Y'_i \\  Y' \end{bmatrix} + 0\)</span></p>
        <p><span class="arithmatex">\(= P_{i+k} \left( \lambda^k X'_i~^\intercal W'_i Y'_i + X'~^\intercal W' Y' \right)\)</span></p>
        <p><span class="arithmatex">\(= P_{i+k} \left(\lambda^k X'_i~^\intercal W'_i Y'_i + X~^\intercal W Y \right)\)</span></p>
        <p>Since the above expression still depends on our earlier data matrices through the expression <span class="arithmatex">\(X'_i~^\intercal W'_i Y'_i\)</span>, we need to massage this a bit further. We note the following. </p>
        <p><span class="arithmatex">\({1 \over \lambda^k} (R_{i+k} - R) = R_i\)</span> (since <span class="arithmatex">\(R_{i+k} = \lambda^k R_i + R\)</span>)</p>
        <p><span class="arithmatex">\(X'_i~^\intercal W'_i Y'_i = X_i^\intercal W_i Y_i = R_i R_i^{-1}X_i^\intercal W_i Y_i = R_i \pmb{\theta}_i = {1 \over \lambda^k} (R_{i+k} - R) \pmb{\theta}_i = {1 \over \lambda^k} (R_{i+k} - X^\intercal W X) \pmb{\theta}_i\)</span></p>
        <p>By plugging this equivalent statement in, we are able to drop the dependence on all previous state except the precision matrix and parameter vector.</p>
        <p><span class="arithmatex">\(\pmb{\theta}_{i+k} = P_{i+k} \left(\lambda^k X'_i~^\intercal W'_i Y'_i + X~^\intercal W Y \right)\)</span></p>
        <p><span class="arithmatex">\(= P_{i+k} \left( \lambda^k \left( {1 \over \lambda^k} (R_{i+k} - X^\intercal W X) \pmb{\theta}_i \right) + X~^\intercal W Y \right)\)</span></p>
        <p><span class="arithmatex">\(= P_{i+k}\left((R_{i+k} - X^\intercal W X) \pmb{\theta}_i + X~^\intercal W Y\right)\)</span></p>
        <p><span class="arithmatex">\(= (I - P_{i+k}X^\intercal W X) \pmb{\theta}_i + P_{i+k} X~^\intercal W Y\)</span></p>
        <p><span class="arithmatex">\(= \pmb{\theta}_i - P_{i+k}X^\intercal W X \pmb{\theta}_i + P_{i+k} X~^\intercal W Y\)</span></p>
        <p><span class="arithmatex">\(= \pmb{\theta}_i + P_{i+k}X^\intercal W (Y - X \pmb{\theta}_i)\)</span></p>
        <h3>Faster recursive least squares</h3>
        <p>Now we can update <span class="arithmatex">\(\pmb{\theta}\)</span> after <span class="arithmatex">\(k\)</span> examples by recursively updating the covariance matrix, inverting it, and recursively updating our parameter vector.</p>
        <p><span class="arithmatex">\(R_{i+k} = \lambda^k R_i + R\)</span></p>
        <p><span class="arithmatex">\(\pmb{\theta}_{i+k} = \pmb{\theta}_i + R_{i+k}^{-1} X^\intercal W (Y - X \pmb{\theta}_i)\)</span></p>
        <p>Using this approach, we incur matrix multiplication computation complexity of <span class="arithmatex">\(\mathcal{O}(kd^2 + k^2d)\)</span> and matrix inverse complexity of <span class="arithmatex">\(\mathcal{O}(d^3)\)</span>. When <span class="arithmatex">\(d &gt; k\)</span>, inverting the covariance matrix dominates. Can we improve this? Yes! We can actually speed things up with a cleverer way of updating the precision matrix. If we apply the <a href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">Woodbury matrix identity</a> to the update of the precision matrix we get the classical <a href="https://en.wikipedia.org/wiki/Recursive_least_squares_filter">recursive least squares</a> algorithm. Not only is this <span class="arithmatex">\(\mathcal{O}(kd^2 + k^2d + k^3)\)</span> instead of <span class="arithmatex">\(\mathcal{O}(d^3 + kd^2 + k^2d)\)</span>, but it's also proven to be numerically stable when run on a computer.</p>
        <p>The Woodbury matrix identity states that when we have already computed the inverse <span class="arithmatex">\(A_0^{-1}\)</span> of matrix <span class="arithmatex">\(A_0\)</span>, and we wish to compute the inverse of an updated <span class="arithmatex">\(A = A_0 + UCV\)</span>, we can do so efficiently via the following equation.</p>
        <p><span class="arithmatex">\(A^{-1} = A_0^{-1} - A_0^{-1}U(C^{-1} + VA_0^{-1}U)^{-1}VA_0^{-1}\)</span>.</p>
        <p>For "thin" <span class="arithmatex">\(U\)</span> and "short" <span class="arithmatex">\(V\)</span>, this results in much smaller matrix operations than the direct inversion of <span class="arithmatex">\(A\)</span>. </p>
        <p>If we squint at our equations above, we see there is a match between the covariance matrix update and the Woodberry equation.</p>
        <p><span class="arithmatex">\(A = R_{i+k}, A_0 = \lambda^k R_i, U = X^\intercal, C = W, V = X\)</span></p>
        <p><span class="arithmatex">\(A = R_{i+k} = A_0 + UCV = \lambda^k R_i + X^\intercal W X\)</span></p>
        <p>Plugging into the Woodberry result, we get an update for <span class="arithmatex">\(P_{i+k}\)</span>.</p>
        <p><span class="arithmatex">\(A^{-1} = R_{i+k}^{-1} = P_{i+k} = A_0^{-1} - A_0^{-1}U(C^{-1} + VA_0^{-1}U)^{-1}VA_0^{-1}\)</span></p>
        <p><span class="arithmatex">\(=(\lambda^k R_i)^{-1} - (\lambda^k R_i)^{-1} X^\intercal (W^{-1} + X (\lambda^k R_i)^{-1} X^\intercal)^{-1} X (\lambda^k R_i)^{-1}\)</span></p>
        <p><span class="arithmatex">\(={1 \over \lambda^k} P_i - {1 \over \lambda^k} P_i X^\intercal (W^{-1} + X {1 \over \lambda^k} P_i X^\intercal)^{-1} X {1 \over \lambda^k} P_i\)</span></p>
        <p>We can clean this up a bit. Let's start by defining a new <span class="arithmatex">\(U\)</span> matrix so that <span class="arithmatex">\(W^{-1} = {1 \over \lambda^k} U\)</span>. Let's also look at how to construct <span class="arithmatex">\(U\)</span>.</p>
        <p><span class="arithmatex">\(\pmb{u'} = \begin{bmatrix}
        \lambda &amp; \lambda^2 &amp; \cdots &amp; \lambda^k
        \end{bmatrix}^\intercal = {\lambda^k \over \pmb{w'}}\)</span></p>
        <p><span class="arithmatex">\(\mu = {\lambda^k \over \beta \sum_{t=1}^k\lambda^{k-t}} = {\lambda^k \over v_k}\)</span>, <span class="arithmatex">\(\pmb{\mu} = \begin{bmatrix}
        \mu &amp; \mu &amp; \cdots &amp; \mu
        \end{bmatrix}^\intercal\)</span>, <span class="arithmatex">\(\pmb{\mu} \in \mathbb{R}^{d}\)</span></p>
        <p><span class="arithmatex">\(\pmb{u} = \begin{bmatrix} \pmb{u}' \\ \pmb{\mu} \end{bmatrix} = {\lambda^k \over \pmb{w}}\)</span>, <span class="arithmatex">\(U = \operatorname{diag}(\pmb{u}) = \lambda^k W^{-1}\)</span></p>
        <p>Using <span class="arithmatex">\(U\)</span> our update for the precision matrix simplifies nicely.</p>
        <p><span class="arithmatex">\(P_{i+k} = {1 \over \lambda^k} P_i - {1 \over \lambda^k} P_i X^\intercal (W^{-1} + X {1 \over \lambda^k} P_i X^\intercal)^{-1} X {1 \over \lambda^k} P_i\)</span></p>
        <p><span class="arithmatex">\(= {1 \over \lambda^k} P_i - {1 \over \lambda^k} P_i X^\intercal ({1 \over \lambda^k} U + X {1 \over \lambda^k} P_i X^\intercal)^{-1} X {1 \over \lambda^k} P_i\)</span></p>
        <p><span class="arithmatex">\(= {1 \over \lambda^k} P_i - {1 \over \lambda^k} P_i X^\intercal \lambda^k(U + X P_i X^\intercal)^{-1} X {1 \over \lambda^k} P_i\)</span></p>
        <p><span class="arithmatex">\(= {1 \over \lambda^k} \left( P_i - P_i X^\intercal (U + X P_i X^\intercal)^{-1} X P_i \right)\)</span></p>
        <p>Here we see that the only matrix inverse remaining is now <span class="arithmatex">\(\mathcal{O}(k^3)\)</span>, with the matrix multiplications still <span class="arithmatex">\(\mathcal{O}(kd^2 + k^2d)\)</span>. Success!</p>
        <!-- EMD GENERATED FROM MARKDOWN -->
    </article>

    <!-- MathJax for Math Typesetting -->
    <script src="https://lukemerrick.com/static/load-mathjax.js" async></script>
</body>

</html>